{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading address mapping...\n",
      "Total addresses loaded from mapping: 48298522\n",
      "Sample address mappings: [(42313618, 'dd9fd6b6f8f7ea932997992bbe67eabb3e316f3c'), (42313619, '689c56aef474df92d44a1b70850f808488f9769c'), (42313620, 'b64ffdca47d6c3895608c4e05faba6e617b3a031'), (42313621, '816051e2203ca534c4336d8d6df71987fa3ae0bd'), (42313622, '5a4e849325e9b87bdb918bbe1bbafdea819bbefe')]\n",
      "\n",
      "Processing CCG files for targets...\n",
      "Processing file: 32\n",
      "Processing file: 35\n",
      "Processing file: 34\n",
      "Processing file: 33\n",
      "Processing file: 20\n",
      "Processing file: 18\n",
      "Processing file: 27\n",
      "Processing file: 9\n",
      "Processing file: 0\n",
      "Processing file: 11\n",
      "Processing file: 7\n",
      "Processing file: 29\n",
      "Processing file: 16\n",
      "Processing file: 6\n",
      "Processing file: 28\n",
      "Processing file: 17\n",
      "Processing file: 1\n",
      "Processing file: 10\n",
      "Processing file: 19\n",
      "Processing file: 26\n",
      "Processing file: 8\n",
      "Processing file: 21\n",
      "Processing file: 38\n",
      "Processing file: 36\n",
      "Processing file: 31\n",
      "Processing file: 30\n",
      "Processing file: 37\n",
      "Processing file: 24\n",
      "Processing file: 23\n",
      "Processing file: 4\n",
      "Processing file: 15\n",
      "Processing file: 3\n",
      "Processing file: 12\n",
      "Processing file: 2\n",
      "Processing file: 13\n",
      "Processing file: 5\n",
      "Processing file: 14\n",
      "Processing file: 22\n",
      "Processing file: 25\n",
      "\n",
      "Unique mapped indices: 8753232\n",
      "Loading best_dataset.csv...\n",
      "Normalizing addresses...\n",
      "Filtering dataset by mapped indices...\n",
      "\n",
      "Aggregated Dataset Summary:\n",
      "Total unique smart contracts: 3342\n",
      "Number of Ponzi contracts (target=1): 504\n",
      "Number of Non-Ponzi contracts (target=0): 2838\n",
      "Aggregated dataset saved to 'aggregated_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to datasets\n",
    "alladdress_path = \"../dataset/graph analysis/TOIT graph data/alladdress\"\n",
    "ccg_folder_path = \"../dataset/graph analysis/net-001/CCG\"\n",
    "best_dataset_path = \"../dataset/best_dataset.csv\"\n",
    "\n",
    "# Step 1: Load the alladdress mapping\n",
    "print(\"Loading address mapping...\")\n",
    "address_map = {}\n",
    "try:\n",
    "    with open(alladdress_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                address, index = line.strip().split('#')\n",
    "                address_map[int(index)] = address  # Map index (as int) to address\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing line in alladdress: {line.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {alladdress_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error while loading alladdress: {e}\")\n",
    "\n",
    "print(f\"Total addresses loaded from mapping: {len(address_map)}\")\n",
    "print(f\"Sample address mappings: {list(address_map.items())[:5]}\")\n",
    "\n",
    "# Step 2: Process CCG files to extract targets and map them to indices in alladdress\n",
    "print(\"\\nProcessing CCG files for targets...\")\n",
    "mapped_indices = set()\n",
    "\n",
    "try:\n",
    "    for file_name in os.listdir(ccg_folder_path):\n",
    "        file_path = os.path.join(ccg_folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        _, target, _ = line.strip().split('#')  # Extract target\n",
    "                        target_index = int(target)  # Convert target to integer\n",
    "\n",
    "                        # Check if target index exists in address_map\n",
    "                        if target_index in address_map:\n",
    "                            mapped_indices.add(target_index)\n",
    "                    except ValueError:\n",
    "                        print(f\"Error parsing line in {file_name}: {line.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder not found: {ccg_folder_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error while processing CCG files: {e}\")\n",
    "\n",
    "# Results\n",
    "print(f\"\\nUnique mapped indices: {len(mapped_indices)}\")\n",
    "\n",
    "# Step 3: Load best_dataset.csv\n",
    "print(\"Loading best_dataset.csv...\")\n",
    "try:\n",
    "    best_dataset = pd.read_csv(best_dataset_path)\n",
    "\n",
    "    # Normalize the 'address' column by removing the '0x' prefix\n",
    "    print(\"Normalizing addresses...\")\n",
    "    best_dataset['normalized_address'] = best_dataset['address'].str[2:]  # Remove '0x'\n",
    "\n",
    "    # Filter the best_dataset to include only addresses in the mapped indices\n",
    "    print(\"Filtering dataset by mapped indices...\")\n",
    "    best_dataset['index'] = best_dataset['normalized_address'].map(\n",
    "        {v: k for k, v in address_map.items()}\n",
    "    )  # Map normalized_address to indices\n",
    "\n",
    "    aggregated_dataset = best_dataset[\n",
    "        best_dataset['index'].isin(mapped_indices)\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Count total contracts and Ponzi contracts\n",
    "    total_contracts = len(aggregated_dataset)\n",
    "    ponzi_contracts = aggregated_dataset[aggregated_dataset['target'] == 1].shape[0]\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\nAggregated Dataset Summary:\")\n",
    "    print(f\"Total unique smart contracts: {total_contracts}\")\n",
    "    print(f\"Number of Ponzi contracts (target=1): {ponzi_contracts}\")\n",
    "    print(f\"Number of Non-Ponzi contracts (target=0): {total_contracts - ponzi_contracts}\")\n",
    "\n",
    "    # Save aggregated dataset to a new CSV\n",
    "    aggregated_dataset.to_csv(\"aggregated_dataset.csv\", index=False)\n",
    "    print(\"Aggregated dataset saved to 'aggregated_dataset.csv'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {best_dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Degree Neighbors Agreegation\n",
    "mapped index to actual adress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the files\n",
    "alladdress_input_file = \"../dataset/graph analysis/TOIT graph data/alladdress\"  # Replace with actual filename if different\n",
    "alladdress_output_file = \"../dataset/alladdress.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alladdress file saved as: alladdress.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the alladdress document and split into two columns\n",
    "with open(alladdress_input_file, \"r\") as f:\n",
    "    alladdress_lines = f.readlines()\n",
    "\n",
    "# Create a list of dictionaries for easy conversion to a DataFrame\n",
    "alladdress_data = []\n",
    "for line in alladdress_lines:\n",
    "    if \"#\" in line:  # Ensure the line contains valid data\n",
    "        address, index = line.strip().split(\"#\")\n",
    "        alladdress_data.append({\"address\": address, \"index\": int(index)})\n",
    "\n",
    "# Convert to a pandas DataFrame and save as CSV\n",
    "import pandas as pd\n",
    "alladdress_df = pd.DataFrame(alladdress_data)\n",
    "alladdress_df.to_csv(alladdress_output_file, index=False)\n",
    "print(f\"Alladdress file saved as: {alladdress_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alladdress_csv = \"../dataset/alladdress.csv\"\n",
    "txd_csv = \"../dataset/first degree/txd.csv\"\n",
    "output_file = \"../dataset/first degree/txd_to_address.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alladdress_df = pd.read_csv(alladdress_csv)  \n",
    "txfd = pd.read_csv(txd_csv)  \n",
    "\n",
    "# Step 3: Create a mapping dictionary\n",
    "index_to_address = dict(zip(alladdress_df[\"index\"], alladdress_df[\"address\"]))\n",
    "\n",
    "# Step 4: Map source and target indices to addresses\n",
    "txfd[\"source\"] = txfd[\"source\"].map(index_to_address)\n",
    "txfd[\"target\"] = txfd[\"target\"].map(index_to_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped transaction file saved at: ../dataset/first degree/txd_to_address.csv\n"
     ]
    }
   ],
   "source": [
    "txfd.to_csv(output_file, index=False)\n",
    "print(f\"Mapped transaction file saved at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>109c4f2ccc82c4d77bde15f306707320294aea3f</td>\n",
       "      <td>881b0a4e9c55d08e31d8d3c022144d75a454211c</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>109c4f2ccc82c4d77bde15f306707320294aea3f</td>\n",
       "      <td>fd2605a2bf58fdbb90db1da55df61628b47f9e8c</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109c4f2ccc82c4d77bde15f306707320294aea3f</td>\n",
       "      <td>834e9b529ac9fa63b39a06f8d8c9b0d6791fa5df</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>109c4f2ccc82c4d77bde15f306707320294aea3f</td>\n",
       "      <td>17580b766f7453525ca4c6a88b01b50570ea088c</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>109c4f2ccc82c4d77bde15f306707320294aea3f</td>\n",
       "      <td>f88a65846c19d8fc76fff545feaa7bbc7114f667</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    source  \\\n",
       "0           0  109c4f2ccc82c4d77bde15f306707320294aea3f   \n",
       "1           1  109c4f2ccc82c4d77bde15f306707320294aea3f   \n",
       "2           2  109c4f2ccc82c4d77bde15f306707320294aea3f   \n",
       "3           3  109c4f2ccc82c4d77bde15f306707320294aea3f   \n",
       "4           4  109c4f2ccc82c4d77bde15f306707320294aea3f   \n",
       "\n",
       "                                     target  value  \n",
       "0  881b0a4e9c55d08e31d8d3c022144d75a454211c  260.0  \n",
       "1  fd2605a2bf58fdbb90db1da55df61628b47f9e8c   82.0  \n",
       "2  834e9b529ac9fa63b39a06f8d8c9b0d6791fa5df  156.0  \n",
       "3  17580b766f7453525ca4c6a88b01b50570ea088c  206.0  \n",
       "4  f88a65846c19d8fc76fff545feaa7bbc7114f667  302.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txfd.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
